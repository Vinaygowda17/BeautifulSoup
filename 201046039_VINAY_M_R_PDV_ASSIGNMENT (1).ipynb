{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "201046039_VINAY M.R_PDV_ASSIGNMENT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyauTuTpmUoj"
      },
      "source": [
        "pip install fake-useragent "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P0lpNiHjf2L"
      },
      "source": [
        "import requests\n",
        "import bs4\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "from urllib.request import urlopen\n",
        "\n",
        "#create fake user agent\n",
        "ua = UserAgent()\n",
        "header = {'User-Agent':str(ua.chrome)}\n",
        "url = 'https://news.google.com/news/rss'\n",
        "op = urlopen(url)\n",
        "rd = op.read()\n",
        "op.close()\n",
        "soup = bs4.BeautifulSoup(rd,'xml')\n",
        "news_list = soup.find_all('item')\n",
        "\n",
        "#create a dictionary\n",
        "d = {'key':'value'}\n",
        "print(d)\n",
        "\n",
        "#update a dictionary\n",
        "d['newkey'] = 'newvalue'\n",
        "print(d)\n",
        "\n",
        "#create a dictionary\n",
        "live_news = {}\n",
        "\n",
        "news_no = 0\n",
        "\n",
        "for news in news_list:\n",
        "  news_no = news_no + 1\n",
        "  print('Headlines:',news.title.text)\n",
        "  print('Link:',news.link.text)\n",
        "  print('Date and time of Published:',news.pubDate.text)\n",
        "  live_news[news_no] = [news.title.text, news.link.text, news.pubDate.text]\n",
        "\n",
        "print('Total_news:',news_no)    #to print total number of live news displayed\n",
        "\n",
        "#to place the scrapped data into the tables in thier respective rows and columns\n",
        "live_news_df = pd.DataFrame.from_dict(live_news, orient = 'index', columns = ['Headlines','Link','Date and time Published'])\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR1I0iwAhxYl"
      },
      "source": [
        "live_news_df.head() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tng9Orkphzt4"
      },
      "source": [
        "live_news_df.to_csv('live_news.csv') #to store the scrapped data into the .csv files"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}